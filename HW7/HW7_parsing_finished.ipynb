{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW7_parsing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chantmk/NLP_2021/blob/main/HW7/HW7_parsing_finished.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB0lBzkJy_l6"
      },
      "source": [
        "# HW7: Neural Transition-Based Dependency Parsing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwbJPnBmy_l-"
      },
      "source": [
        "In this exercise, you are going to build a deep learning model for Neural Networks Transition-Based Dependency Parsing. A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between “head” words and words which modify those heads. Your implementation will be a transition-based parser, which incrementally builds up a parse one step at a time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTop7w0Wy_mT"
      },
      "source": [
        "To complete this exercise, you will need to complete the code and build a deep learning model for dependency parsing. We will evaluate the model on the subset of Penn Treebank (annotated with Universal Dependencies). \n",
        "\n",
        "We provide the code for data preparation and the skeleton for PartialParse class. You do not need to understand the code outside of this notebook. \n",
        "\n",
        "This homework and the starter codes are adopt from Stanford University class CS224n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RJMmaCSy_mV"
      },
      "source": [
        "This homework does not require you to use Google Colab or Google Cloud as the model is quite small (but you can still use it if you want)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzF49xKny_mX"
      },
      "source": [
        "## 1. Transition-Based Dependency Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwH68mjMy_mY"
      },
      "source": [
        "Your implementation will be a transition-based parser, which incrementally builds\n",
        "up a parse one step at a time. At every step it maintains a partial parse, which is represented as follows:\n",
        "- A stack of words that are currently being processed.\n",
        "- A buffer of words yet to be processed.\n",
        "- A list of dependencies predicted by the parser.\n",
        "\n",
        "Initially, the stack only contains ROOT, the dependencies lists is empty, and the buffer contains all words\n",
        "of the sentence in order. At each step, the parse applies a transition to the partial parse until its buffer is\n",
        "empty and the stack is size 1. The following transitions can be applied:\n",
        "- SHIFT: removes the first word from the buffer and pushes it onto the stack.\n",
        "- LEFT-ARC: marks the second (second most recently added) item on the stack as a dependent of the\n",
        "first item and removes the second item from the stack.\n",
        "- RIGHT-ARC: marks the first (most recently added) item on the stack as a dependent of the second\n",
        "item and removes the first item from the stack.\n",
        "\n",
        "Your parser will decide among transitions at each state using a neural network classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab3kn2OVy_mb"
      },
      "source": [
        "### TODO 1 (Written):\n",
        "Go through the sequence of transitions needed for parsing the sentence “I parsed\n",
        "this sentence correctly”. The dependency tree for the sentence is shown below. At each step, give the\n",
        "configuration of the stack and buffer, as well as what transition was applied this step and what new\n",
        "dependency was added (if any). The first three steps are provided below as an example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTkSbSf6y_ma"
      },
      "source": [
        "<img src=\"https://github.com/ekapolc/NLP_2020/blob/master/HW7/images/img1.jpg?raw=true\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPmRQbXky_mc"
      },
      "source": [
        "Complete the following table (double click the table and fill in the rest):\n",
        "\n",
        "| stack    |  buffer |  new dependency | transition |\n",
        "| :------: |:------: | :-------------: | :--------: |\n",
        "| \\[ROOT\\]            | \\[I, parsed, this, sentence, correctly\\] |          | Initial Configuration |\n",
        "| \\[ROOT, I\\]         | \\[parsed, this, sentence, correctly\\]    |          | SHIFT |\n",
        "| \\[ROOT, I, parsed\\] | \\[this, sentence, correctly\\]            |          | SHIFT |\n",
        "| \\[ROOT, parsed\\]    | \\[this, sentence, correctly\\]            | parsed → I | LEFT-ARC |\n",
        "| \\[ROOT, parsed, this\\] | \\[sentence, correctly\\] | | SHIFT |\n",
        "| \\[ROOT, parsed, this, sentence\\]| [correctly\\]| | SHIFT |\n",
        "| \\[ROOT, parsed, sentence\\] | [correctly\\] | sentence → this | LEFt-ARC |\n",
        "| \\[ROOT, parsed\\] | [correctly\\] | parsed → sentence | RIGHT-ARC |\n",
        "| \\[ROOT, parsed, correctly\\] | [\\] | | SHIFT |\n",
        "| \\[ROOT, parsed\\] | [\\] | parsed → correctly | RIGHT-ARC |\n",
        "| \\[ROOT\\] | [\\] | ROOT → parsed | RIGHT-ARC |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h6PmOd6y_me"
      },
      "source": [
        "### TODO 2 (Coding):\n",
        "Implement the __\\_\\_init\\_\\___ and __parse_step__ functions in the PartialParse class. Your code must past both of the following tests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXhOjsN_y_mf"
      },
      "source": [
        "class PartialParse(object):\n",
        "    def __init__(self, sentence):\n",
        "        \"\"\"Initializes this partial parse.\n",
        "\n",
        "        Your code should initialize the following fields:\n",
        "            self.stack: The current stack represented as a list with the top of the stack as the\n",
        "                        last element of the list.\n",
        "            self.buffer: The current buffer represented as a list with the first item on the\n",
        "                         buffer as the first item of the list\n",
        "            self.dependencies: The list of dependencies produced so far. Represented as a list of\n",
        "                    tuples where each tuple is of the form (head, dependent).\n",
        "                    Order for this list doesn't matter.\n",
        "\n",
        "        The root token should be represented with the string \"ROOT\"\n",
        "\n",
        "        Args:\n",
        "            sentence: The sentence to be parsed as a list of words.\n",
        "                      Your code should not modify the sentence.\n",
        "        \"\"\"\n",
        "        # The sentence being parsed is kept for bookkeeping purposes. Do not use it in your code.\n",
        "        self.sentence = sentence\n",
        "\n",
        "        ### YOUR CODE HERE\n",
        "        self.stack = [\"ROOT\"]\n",
        "        self.buffer = list(sentence)\n",
        "        self.dependencies = []\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def parse_step(self, transition):\n",
        "        \"\"\"Performs a single parse step by applying the given transition to this partial parse\n",
        "\n",
        "        Args:\n",
        "            transition: A string that equals \"S\", \"LA\", or \"RA\" representing the shift, left-arc,\n",
        "                        and right-arc transitions. You can assume the provided transition is a legal\n",
        "                        transition.\n",
        "        Return:\n",
        "            True if transition is legal\n",
        "            Exception if transition is illegal\n",
        "            False if base case\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE\n",
        "        # if len(self.buffer) == 0 and len(self.stack) == 1: \n",
        "        #     return False\n",
        "        # elif len(self.buffer) == 0 and len(self.stack) == 2:\n",
        "        #     self.dependencies.append((self.stack[-2], self.stack[-1]))\n",
        "        #     self.stack.pop(-1)\n",
        "        # elif transition == \"S\" and len(self.buffer) != 0:\n",
        "        #     self.stack.append(self.buffer[0])\n",
        "        #     self.buffer = self.buffer[1:]\n",
        "        # elif transition == \"LA\" and len(self.stack) >= 1:\n",
        "        #     self.dependencies.append((self.stack[-1], self.stack[-2]))\n",
        "        #     self.stack.pop(-2)\n",
        "        # elif transition == \"RA\" and len(self.stack) >= 1:\n",
        "        #     self.dependencies.append((self.stack[-2], self.stack[-1]))\n",
        "        #     self.stack.pop(-1)\n",
        "        # else: \n",
        "        #     raise Exception(\"My Error: Transition illegal: {}, Buffer size: {}, Stack size: {}\".format(transition, len(self.buffer), len(self.stack)))\n",
        "        # return True\n",
        "        if transition == \"S\":\n",
        "            self.stack.append(self.buffer[0])\n",
        "            self.buffer = self.buffer[1:]\n",
        "        elif transition == \"LA\":\n",
        "            self.dependencies.append((self.stack[-1], self.stack[-2]))\n",
        "            self.stack.pop(-2)\n",
        "        elif transition == \"RA\":\n",
        "            self.dependencies.append((self.stack[-2], self.stack[-1]))\n",
        "            self.stack.pop(-1)\n",
        "        else: \n",
        "            raise Exception(\"My Error: Transition illegal: {}, Buffer size: {}, Stack size: {}\".format(transition, len(self.buffer), len(self.stack)))\n",
        "        return True\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def parse(self, transitions):\n",
        "        \"\"\"Applies the provided transitions to this PartialParse\n",
        "\n",
        "        Args:\n",
        "            transitions: The list of transitions in the order they should be applied\n",
        "        Returns:\n",
        "            dependencies: The list of dependencies produced when parsing the sentence. Represented\n",
        "                          as a list of tuples where each tuple is of the form (head, dependent)\n",
        "        \"\"\"\n",
        "        for transition in transitions:\n",
        "            self.parse_step(transition)\n",
        "        return self.dependencies\n",
        "    \n",
        "    def __str__(self):\n",
        "        return \"Dependencies: {}, Stack: {}, Buffer: {}\".format(self.dependencies, self.stack, self.buffer)\n",
        "    def __rpr__(self):\n",
        "        return \"test\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kF_8ESPy_mk"
      },
      "source": [
        "# Do not modify this code\n",
        "def test_step(name, transition, stack, buf, deps,\n",
        "              ex_stack, ex_buf, ex_deps):\n",
        "    \"\"\"Tests that a single parse step returns the expected output\"\"\"\n",
        "    pp = PartialParse([])\n",
        "    pp.stack, pp.buffer, pp.dependencies = stack, buf, deps\n",
        "\n",
        "    pp.parse_step(transition)\n",
        "    stack, buf, deps = (tuple(pp.stack), tuple(pp.buffer), tuple(sorted(pp.dependencies)))\n",
        "    assert stack == ex_stack, \\\n",
        "        \"{:} test resulted in stack {:}, expected {:}\".format(name, stack, ex_stack)\n",
        "    assert buf == ex_buf, \\\n",
        "        \"{:} test resulted in buffer {:}, expected {:}\".format(name, buf, ex_buf)\n",
        "    assert deps == ex_deps, \\\n",
        "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
        "    print(\"{:} test passed!\".format(name))\n",
        "\n",
        "\n",
        "def test_parse_step():\n",
        "    \"\"\"Simple tests for the PartialParse.parse_step function\n",
        "    Warning: these are not exhaustive\n",
        "    \"\"\"\n",
        "    test_step(\"SHIFT\", \"S\", [\"ROOT\", \"the\"], [\"cat\", \"sat\"], [],\n",
        "              (\"ROOT\", \"the\", \"cat\"), (\"sat\",), ())\n",
        "    test_step(\"LEFT-ARC\", \"LA\", [\"ROOT\", \"the\", \"cat\"], [\"sat\"], [],\n",
        "              (\"ROOT\", \"cat\",), (\"sat\",), ((\"cat\", \"the\"),))\n",
        "    test_step(\"RIGHT-ARC\", \"RA\", [\"ROOT\", \"run\", \"fast\"], [], [],\n",
        "              (\"ROOT\", \"run\",), (), ((\"run\", \"fast\"),))\n",
        "\n",
        "\n",
        "def test_parse():\n",
        "    \"\"\"Simple tests for the PartialParse.parse function\n",
        "    Warning: these are not exhaustive\n",
        "    \"\"\"\n",
        "    sentence = [\"parse\", \"this\", \"sentence\"]\n",
        "    dependencies = PartialParse(sentence).parse([\"S\", \"S\", \"S\", \"LA\", \"RA\", \"RA\"])\n",
        "    dependencies = tuple(sorted(dependencies))\n",
        "    expected = (('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this'))\n",
        "    assert dependencies == expected,  \\\n",
        "        \"parse test resulted in dependencies {:}, expected {:}\".format(dependencies, expected)\n",
        "    assert tuple(sentence) == (\"parse\", \"this\", \"sentence\"), \\\n",
        "        \"parse test failed: the input sentence should not be modified\"\n",
        "    print(\"parse test passed!\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwNZjFgey_mn",
        "outputId": "aa240aca-67c9-4c46-b5ad-50276c8f1a1f"
      },
      "source": [
        "test_parse_step()\n",
        "test_parse()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SHIFT test passed!\n",
            "LEFT-ARC test passed!\n",
            "RIGHT-ARC test passed!\n",
            "parse test passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lx3xP_Zy_mr"
      },
      "source": [
        "### TODO 3 (Coding):\n",
        "Our network will predict which transition should be applied next to a partial parse. We could use it to parse a single sentence by applying predicted transitions until the parse is complete. However, neural networks run much more efficiently when making predictions about batches of data at a time (i.e., predicting the next transition for a many different partial parses simultaneously). We can parse sentences in minibatches with the following algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oTyD-Dcy_ms"
      },
      "source": [
        "<img src=\"https://github.com/ekapolc/NLP_2020/blob/master/HW7/images/img2.jpg?raw=true\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhRpDDXay_mt"
      },
      "source": [
        "Implement this algorithm in the minibatch parse function and run the following test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gio_qpS5y_mu"
      },
      "source": [
        "def minibatch_parse(sentences, model, batch_size):\n",
        "    \"\"\"Parses a list of sentences in minibatches using a model.\n",
        "\n",
        "    Args:\n",
        "        sentences: A list of sentences to be parsed (each sentence is a list of words)\n",
        "        model: The model that makes parsing decisions. It is assumed to have a function\n",
        "               model.predict(partial_parses) that takes in a list of PartialParses as input and\n",
        "               returns a list of transitions predicted for each parse. That is, after calling\n",
        "                   transitions = model.predict(partial_parses)\n",
        "               transitions[i] will be the next transition to apply to partial_parses[i].\n",
        "        batch_size: The number of PartialParses to include in each minibatch\n",
        "    Returns:\n",
        "        dependencies: A list where each element is the dependencies list for a parsed sentence.\n",
        "                      Ordering should be the same as in sentences (i.e., dependencies[i] should\n",
        "                      contain the parse for sentences[i]).\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    partial_parses = [PartialParse(sentence) for sentence in sentences]\n",
        "    unfinished_parses = partial_parses.copy()\n",
        "    dependencies = []\n",
        "    while len(unfinished_parses) > 0:\n",
        "        minibatch = unfinished_parses[:batch_size]\n",
        "        transitions = model.predict(minibatch)\n",
        "\n",
        "        for i in range(len(transitions)):\n",
        "            minibatch[i].parse([transitions[i]])\n",
        "            if len(minibatch[i].buffer) == 0 and len(minibatch[i].stack) == 1:\n",
        "                # dependencies.append(minibatch[i].dependencies)\n",
        "                unfinished_parses.remove(minibatch[i])\n",
        "    for pp in partial_parses:\n",
        "        # if \n",
        "        dependencies.append(pp.dependencies)\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return dependencies"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXNoWwaWy_mx"
      },
      "source": [
        "# Do not modify this code\n",
        "class DummyModel(object):\n",
        "    \"\"\"Dummy model for testing the minibatch_parse function\n",
        "    First shifts everything onto the stack and then does exclusively right arcs if the first word of\n",
        "    the sentence is \"right\", \"left\" if otherwise.\n",
        "    \"\"\"\n",
        "    def predict(self, partial_parses):\n",
        "        return [(\"RA\" if pp.stack[1] is \"right\" else \"LA\") if len(pp.buffer) == 0 else \"S\"\n",
        "                for pp in partial_parses]\n",
        "\n",
        "\n",
        "def test_dependencies(name, deps, ex_deps):\n",
        "    \"\"\"Tests the provided dependencies match the expected dependencies\"\"\"\n",
        "    deps = tuple(sorted(deps))\n",
        "    assert deps == ex_deps, \\\n",
        "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
        "\n",
        "\n",
        "def test_minibatch_parse():\n",
        "    \"\"\"Simple tests for the minibatch_parse function\n",
        "    Warning: these are not exhaustive\n",
        "    \"\"\"\n",
        "    sentences = [[\"right\", \"arcs\", \"only\"],\n",
        "                 [\"right\", \"arcs\", \"only\", \"again\"],\n",
        "                 [\"left\", \"arcs\", \"only\"],\n",
        "                 [\"left\", \"arcs\", \"only\", \"again\"]]\n",
        "    deps = minibatch_parse(sentences, DummyModel(), 2)\n",
        "    test_dependencies(\"minibatch_parse\", deps[0],\n",
        "                      (('ROOT', 'right'), ('arcs', 'only'), ('right', 'arcs')))\n",
        "    test_dependencies(\"minibatch_parse\", deps[1],\n",
        "                      (('ROOT', 'right'), ('arcs', 'only'), ('only', 'again'), ('right', 'arcs')))\n",
        "    test_dependencies(\"minibatch_parse\", deps[2],\n",
        "                      (('only', 'ROOT'), ('only', 'arcs'), ('only', 'left')))\n",
        "    test_dependencies(\"minibatch_parse\", deps[3],\n",
        "                      (('again', 'ROOT'), ('again', 'arcs'), ('again', 'left'), ('again', 'only')))\n",
        "    print(\"minibatch_parse test passed!\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-13VeiKy_m1",
        "outputId": "55b0be27-17a4-4046-d583-c3060a4019fa"
      },
      "source": [
        "test_minibatch_parse()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "minibatch_parse test passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL5kIWKXy_m4"
      },
      "source": [
        "## 2. Setup and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE4cYvf1gfe9",
        "outputId": "e3d397ed-abae-4555-fba6-29f56353c643"
      },
      "source": [
        "!wget --no-check-certificate https://raw.githubusercontent.com/ekapolc/nlp_2020/master/HW7/HW7.zip"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-05 13:10:17--  http://wget/\n",
            "Resolving wget (wget)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘wget’\n",
            "--2021-03-05 13:10:17--  https://raw.githubusercontent.com/ekapolc/nlp_2020/master/HW7/HW7.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 39003291 (37M) [application/zip]\n",
            "Saving to: ‘HW7.zip’\n",
            "\n",
            "HW7.zip             100%[===================>]  37.20M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-03-05 13:10:18 (259 MB/s) - ‘HW7.zip’ saved [39003291/39003291]\n",
            "\n",
            "FINISHED --2021-03-05 13:10:18--\n",
            "Total wall clock time: 0.4s\n",
            "Downloaded: 1 files, 37M in 0.1s (259 MB/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mogV7JQ0qIQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b426a3-5ea9-4c4e-ed3c-aca62be61202"
      },
      "source": [
        "!unzip HW7.zip"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  HW7.zip\n",
            "  inflating: HW7_parsing.ipynb       \n",
            "   creating: data/\n",
            "  inflating: data/dev.conll          \n",
            "  inflating: data/dev.gold.conll     \n",
            "  inflating: data/en-cw.txt          \n",
            "  inflating: data/test.conll         \n",
            "  inflating: data/test.gold.conll    \n",
            "  inflating: data/train.conll        \n",
            "  inflating: data/train.gold.conll   \n",
            "   creating: utils/\n",
            "   creating: utils/.ipynb_checkpoints/\n",
            "  inflating: utils/.ipynb_checkpoints/general_utils-checkpoint.py  \n",
            "  inflating: utils/.ipynb_checkpoints/parser_utils-checkpoint.py  \n",
            "  inflating: utils/general_utils.py  \n",
            "  inflating: utils/parser_utils.py   \n",
            " extracting: utils/__init__.py       \n",
            "   creating: utils/__pycache__/\n",
            "  inflating: utils/__pycache__/general_utils.cpython-35.pyc  \n",
            "  inflating: utils/__pycache__/parser_utils.cpython-35.pyc  \n",
            "  inflating: utils/__pycache__/__init__.cpython-35.pyc  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zemNby1uy_m5"
      },
      "source": [
        "from utils.parser_utils import minibatches, load_and_preprocess_data"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RksEEdJvy_m8"
      },
      "source": [
        "Preparing data. We will use a subset of Penn Treebank and pretrained embeddings in this task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XNgGpMUy_m9"
      },
      "source": [
        "We are now going to train a neural network to predict, given the state of the stack, buffer, and dependencies, which transition should be applied next. First, the model extracts a feature vector representing the current state. We will be using the feature set presented in the original neural dependency parsing paper: A Fast and Accurate Dependency Parser using Neural Networks. \n",
        "\n",
        "The function extracting these features has been implemented for you in parser_utils. This feature vector consists of a list of tokens (e.g., the last word in the stack, first word in the buffer, dependent of the second-to-last word in the stack if there is one, etc.). They can be represented as a list of integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhqbGTYpy_m-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b2c788f-669c-4900-d914-18ef6b9ed3d1"
      },
      "source": [
        "parser, embeddings, train_examples, dev_set, test_set = load_and_preprocess_data(True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "took 1.86 seconds\n",
            "Building parser...\n",
            "took 0.02 seconds\n",
            "Loading pretrained embeddings...\n",
            "took 1.96 seconds\n",
            "Vectorizing data...\n",
            "took 0.05 seconds\n",
            "Preprocessing training data...\n",
            "took 1.16 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-VYL2rKy_nB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a010a4e3-88dd-406d-a41a-986dff06f575"
      },
      "source": [
        "print(len(train_examples), len(dev_set), len(test_set))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48390 500 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FubWDIsNy_nE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "301e73a2-27d4-4fa5-b3d8-c64bca9ff81c"
      },
      "source": [
        "print(embeddings.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5157, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBD3A4yVy_nI"
      },
      "source": [
        "Get the full batch of our subset data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vNwux9by_nJ"
      },
      "source": [
        "minibatch_gen = minibatches(train_examples, len(train_examples))\n",
        "x_train, y_train = minibatch_gen.__next__()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i__8jliGy_nM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a07b55a8-a92c-4f87-ccd1-d699dc7388b1"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(48390, 36)\n",
            "(48390, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LerZSYby_nP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44259710-8706-4965-a894-8dc88456a798"
      },
      "source": [
        "# Sample features\n",
        "print(x_train[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5156   99 1139   87 5155 5155  101   98  117 5155 5155  264  194 5155\n",
            " 5155 5155 1200 5155   84   48   50   46   83   83   54   39   58   83\n",
            "   83   43   42   83   83   83   42   83]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17eqeESxy_nR"
      },
      "source": [
        "## 3. Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5mqcz1qy_nT"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input, Dense, Reshape, Dropout, Flatten\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sW29Rhvy_nX"
      },
      "source": [
        "### TODO 4 (Coding):\n",
        "Build and train a tensroflow keras model to predict an action for each state of of the input. This is a simple classification task. \n",
        "- The input and output of the model must match the dimention of x_train and y_train.\n",
        "- The model must use the provided pretrained embeddings\n",
        "- The model could comprise of only a feedforward layer and a dropout\n",
        "- Training loss should be around 0.1 or below, and training categorical_accuracy above 0.94"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAWlQI6iy_nY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c62028e5-b0d0-451c-a683-62312096710b"
      },
      "source": [
        "model = Sequential()\n",
        "# Write your code here\n",
        "model.add(Input(shape=(x_train.shape[1])))\n",
        "model.add(Embedding(input_dim=5157, output_dim=50, embeddings_initializer=Constant(embeddings)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(700, activation=\"relu\"))\n",
        "model.add(Dense(500, activation=\"relu\"))\n",
        "model.add(Dense(300, activation=\"relu\"))\n",
        "model.add(Dense(100, activation=\"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y_train.shape[1], activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=Adam(), loss=\"categorical_crossentropy\", metrics=\"categorical_accuracy\")\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 36, 50)            257850    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1800)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1000)              1801000   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 700)               700700    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 500)               350500    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 300)               150300    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 303       \n",
            "=================================================================\n",
            "Total params: 3,290,753\n",
            "Trainable params: 3,290,753\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMjs0W69y_nb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2cc5e29-5b2e-4c76-ab27-968bafe39fe9"
      },
      "source": [
        "# Write your code here\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=10, verbose=1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "379/379 [==============================] - 6s 8ms/step - loss: 0.4742 - categorical_accuracy: 0.7954\n",
            "Epoch 2/10\n",
            "379/379 [==============================] - 3s 8ms/step - loss: 0.1770 - categorical_accuracy: 0.9325\n",
            "Epoch 3/10\n",
            "379/379 [==============================] - 3s 8ms/step - loss: 0.1459 - categorical_accuracy: 0.9461\n",
            "Epoch 4/10\n",
            "379/379 [==============================] - 3s 7ms/step - loss: 0.0906 - categorical_accuracy: 0.9662\n",
            "Epoch 5/10\n",
            "379/379 [==============================] - 3s 7ms/step - loss: 0.0783 - categorical_accuracy: 0.9711\n",
            "Epoch 6/10\n",
            "379/379 [==============================] - 3s 7ms/step - loss: 0.0498 - categorical_accuracy: 0.9827\n",
            "Epoch 7/10\n",
            "379/379 [==============================] - 3s 8ms/step - loss: 0.0345 - categorical_accuracy: 0.9878\n",
            "Epoch 8/10\n",
            "379/379 [==============================] - 3s 8ms/step - loss: 0.0244 - categorical_accuracy: 0.9917\n",
            "Epoch 9/10\n",
            "379/379 [==============================] - 3s 7ms/step - loss: 0.0213 - categorical_accuracy: 0.9934\n",
            "Epoch 10/10\n",
            "379/379 [==============================] - 3s 7ms/step - loss: 0.0146 - categorical_accuracy: 0.9949\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fcfa0106810>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "332SL84Yy_nf"
      },
      "source": [
        "## 4. Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HohiOHZUy_nf"
      },
      "source": [
        "For Dependency Parsing, we usually report attachment score of the model for evaluation. There are two possible metrics UAS and LAS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3hmM0Kly_ng"
      },
      "source": [
        "### TODO 5 (Written and Coding):\n",
        "Explain how attachment score is calculated and the difference between unlabeled attachment score (UAS) and labeled attachment score (LAS). Which one should we use here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rENVVVUhy_nh"
      },
      "source": [
        "__Answer here__: As it described in its name, the labeled attachment score concern that the parser output would suggest label(relation) correctly while both attachment score concern syntatic head. Since we don't have the label that describe the relation here, so we should use UAS\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE2HQT4Vy_ni"
      },
      "source": [
        "Report the score using appropriate metric on dev_set and test_set. The function for calculating scores are provided in parser.parse and the dataset can be passed in as-is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4JJmrsGy_nj"
      },
      "source": [
        "dev_score:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYGSCxBUYgUD"
      },
      "source": [
        "data = dev_set.copy()\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDdl74GVY535"
      },
      "source": [
        "data[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOTPTqf7y_nj"
      },
      "source": [
        "UAS_dev, dep_dev = parser.parse(data, model, minibatch_parse, 512)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91H8i_9akHNM",
        "outputId": "d8b7cd0c-456b-4de3-8d9c-6550fca57bad"
      },
      "source": [
        "print(UAS_dev)\r\n",
        "dep_dev[0]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7403982337748896\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(2, 1),\n",
              " (6, 5),\n",
              " (6, 4),\n",
              " (6, 3),\n",
              " (6, 7),\n",
              " (9, 8),\n",
              " (6, 9),\n",
              " (2, 6),\n",
              " (10, 2),\n",
              " (14, 13),\n",
              " (14, 12),\n",
              " (20, 19),\n",
              " (20, 18),\n",
              " (20, 17),\n",
              " (20, 16),\n",
              " (22, 21),\n",
              " (22, 20),\n",
              " (22, 15),\n",
              " (22, 23),\n",
              " (14, 22),\n",
              " (11, 14),\n",
              " (10, 11),\n",
              " (10, 24),\n",
              " (28, 27),\n",
              " (28, 26),\n",
              " (25, 28),\n",
              " (31, 30),\n",
              " (31, 32),\n",
              " (33, 31),\n",
              " (33, 29),\n",
              " (36, 35),\n",
              " (36, 34),\n",
              " (33, 36),\n",
              " (25, 33),\n",
              " (10, 25),\n",
              " (10, 37),\n",
              " (0, 10)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlCJGdLWy_nm"
      },
      "source": [
        "test_score:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2bJf_1oy_nn",
        "outputId": "03acce20-b646-4628-c6e8-01259b2afb67"
      },
      "source": [
        "UAS_test, dep_test = parser.parse(test_set, model, minibatch_parse, 512)\r\n",
        "print(UAS_test)\r\n",
        "print(dep_test[0])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7531539108494533\n",
            "[(6, 5), (6, 4), (6, 3), (6, 2), (6, 1), (6, 7), (6, 8), (0, 6)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLcAa-fLy_nq"
      },
      "source": [
        "Also, print one sample sentence (in English) in the test set and its predicted dependencies from the model.\n",
        "You can use __parser.id2tok\\[word_id\\]__ to lookup the word in English.\n",
        "\n",
        "__Draw a picture of this sentence with arrows and upload it to my couseville__\n",
        "<img src=\"https://github.com/chantmk/NLP_2021/blob/main/HW7/nlp_parsing.png?raw=true\">\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY6li-u5y_nq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a33540b7-f5bb-46b2-9d33-658c179b240d"
      },
      "source": [
        "sentence = [parser.id2tok[word] for word in test_set[0][\"word\"]]\r\n",
        "sentence"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<ROOT>', 'no', ',', 'it', 'was', \"n't\", 'black', 'monday', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    }
  ]
}