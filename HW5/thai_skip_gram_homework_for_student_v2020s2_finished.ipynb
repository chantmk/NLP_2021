{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "thai_skip_gram_homework_for_student_v2020s2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chantmk/NLP_2021/blob/main/HW5/thai_skip_gram_homework_for_student_v2020s2_finished.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k60eJUVOepzF",
        "outputId": "ef967d89-3492-4228-c41e-93a6969ff2e5"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Feb 20 04:40:07 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    35W / 250W |    417MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnocxlWUWcRW",
        "outputId": "fb832859-fcd0-423a-8296-1acee7e79a45"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko8K6ntRi-W6"
      },
      "source": [
        "# Homework: Word Embedding\n",
        "\n",
        "In this exercise, you will work on the skip-gram neural network architecture for Word2Vec. You will be using Keras to train your model. \n",
        "\n",
        "You must complete the following tasks:\n",
        "1. Read/clean text files\n",
        "2. Indexing (Assign a number to each word)\n",
        "3. Create skip-grams (inputs for your model)\n",
        "4. Create the skip-gram neural network model\n",
        "5. Visualization\n",
        "6. Evaluation (Using pre-trained, not using pre-trained)\n",
        "    (classify topic from 4 categories) \n",
        "    \n",
        "This notebook assumes you have already installed Tensorflow and Keras with python3 and had GPU enabled. If you run this exercise on GCloud using the provided disk image you are all set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw11OhLsi-W8"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import glob\n",
        "import re\n",
        "import random\n",
        "import collections\n",
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import GRU, Dropout\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input, Dense, Masking, Conv1D, Bidirectional\n",
        "from tensorflow.python.keras.layers.merge import Dot\n",
        "from tensorflow.python.keras.utils import np_utils\n",
        "from tensorflow.python.keras.utils.data_utils import get_file\n",
        "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "random.seed(42)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdYpL3Uyi-XD"
      },
      "source": [
        "# Step 1: Read/clean text files\n",
        "\n",
        "The given code can be used to processed the pre-tokenzied text file from the wikipedia corpus. In your homework, you must replace those text files with raw text files.  You must use your own tokenizer to process your text files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wco1eVRVzn6O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dded5fde-1e67-4ba3-e119-92ec4e793bb0"
      },
      "source": [
        "!wget https://www.dropbox.com/s/eexden7246sgfzf/BEST-TrainingSet.zip\n",
        "!wget https://www.dropbox.com/s/n87fiy25f2yc3gt/wiki.zip\n",
        "!unzip wiki.zip\n",
        "!unzip BEST-TrainingSet.zip"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-20 04:40:07--  https://www.dropbox.com/s/eexden7246sgfzf/BEST-TrainingSet.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.64.18, 2620:100:6027:18::a27d:4812\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.64.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/eexden7246sgfzf/BEST-TrainingSet.zip [following]\n",
            "--2021-02-20 04:40:08--  https://www.dropbox.com/s/raw/eexden7246sgfzf/BEST-TrainingSet.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc11877ba0c0f0ce25a81cf93848.dl.dropboxusercontent.com/cd/0/inline/BJSC-DnV7awny3TPHL3dtS6oN6QajW-b4pL6PgZ6rvrL8EtqOKhKPbtvZsdwi-aKhk2w2MIe0hCrUN9GRJnNzC3M5ym39OgtvYWut1bYkft4436UmpMiJjU0ZbpI6wQkq6A/file# [following]\n",
            "--2021-02-20 04:40:08--  https://uc11877ba0c0f0ce25a81cf93848.dl.dropboxusercontent.com/cd/0/inline/BJSC-DnV7awny3TPHL3dtS6oN6QajW-b4pL6PgZ6rvrL8EtqOKhKPbtvZsdwi-aKhk2w2MIe0hCrUN9GRJnNzC3M5ym39OgtvYWut1bYkft4436UmpMiJjU0ZbpI6wQkq6A/file\n",
            "Resolving uc11877ba0c0f0ce25a81cf93848.dl.dropboxusercontent.com (uc11877ba0c0f0ce25a81cf93848.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6027:15::a27d:480f\n",
            "Connecting to uc11877ba0c0f0ce25a81cf93848.dl.dropboxusercontent.com (uc11877ba0c0f0ce25a81cf93848.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BJQgpklgGBbBPk5zOr-07m7Yk-j2dJiUqf3MhClmGoIJSbU7yAB39FffOpuGDrNl4_klc7gFLKDG26McfzBJmC8zFnWcZb3n2SpCNL-qX7YRbCak2iLiTuN0mXu23VfpMIZXEIvF_Q72Res_DsdWzsvqLwHO8XglsOjW5bLTF83_r_V-oJK_u-kF4axb9QOD0MNZvDmeoK__m5kF2L4pqYaG9zcDsaG5ebnHPM2k6rLhXj2V6lGy2dEFNS9HR2NUKK5zJdZ7Guz0387gWSqR5j3Wgbo3qS_riMrYt_Cd9lHHma_kYMgufjdV5bDe1garu9hu-QgVmSda6A1z1MYepPbBjUMPMU_g5uT4GtCUCJ-h5A/file [following]\n",
            "--2021-02-20 04:40:09--  https://uc11877ba0c0f0ce25a81cf93848.dl.dropboxusercontent.com/cd/0/inline2/BJQgpklgGBbBPk5zOr-07m7Yk-j2dJiUqf3MhClmGoIJSbU7yAB39FffOpuGDrNl4_klc7gFLKDG26McfzBJmC8zFnWcZb3n2SpCNL-qX7YRbCak2iLiTuN0mXu23VfpMIZXEIvF_Q72Res_DsdWzsvqLwHO8XglsOjW5bLTF83_r_V-oJK_u-kF4axb9QOD0MNZvDmeoK__m5kF2L4pqYaG9zcDsaG5ebnHPM2k6rLhXj2V6lGy2dEFNS9HR2NUKK5zJdZ7Guz0387gWSqR5j3Wgbo3qS_riMrYt_Cd9lHHma_kYMgufjdV5bDe1garu9hu-QgVmSda6A1z1MYepPbBjUMPMU_g5uT4GtCUCJ-h5A/file\n",
            "Reusing existing connection to uc11877ba0c0f0ce25a81cf93848.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13122229 (13M) [application/zip]\n",
            "Saving to: ‘BEST-TrainingSet.zip.1’\n",
            "\n",
            "BEST-TrainingSet.zi 100%[===================>]  12.51M  9.25MB/s    in 1.4s    \n",
            "\n",
            "2021-02-20 04:40:11 (9.25 MB/s) - ‘BEST-TrainingSet.zip.1’ saved [13122229/13122229]\n",
            "\n",
            "--2021-02-20 04:40:12--  https://www.dropbox.com/s/n87fiy25f2yc3gt/wiki.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6027:18::a27d:4812\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/n87fiy25f2yc3gt/wiki.zip [following]\n",
            "--2021-02-20 04:40:12--  https://www.dropbox.com/s/raw/n87fiy25f2yc3gt/wiki.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uca9ab5a88fbfca1a40dd4f28902.dl.dropboxusercontent.com/cd/0/inline/BJQOH0GC7Y-p6kCWz3fujGlkouWpnS9svoBsfJwYz-_g42bPTE1oA0zPJZGP81w67VPEzlEzlvTDBA4g0JVKQW_dOqqmcH3d9uvSdLpfB4H7ag7Y_H3lbXP-cXyxksEq_y0/file# [following]\n",
            "--2021-02-20 04:40:12--  https://uca9ab5a88fbfca1a40dd4f28902.dl.dropboxusercontent.com/cd/0/inline/BJQOH0GC7Y-p6kCWz3fujGlkouWpnS9svoBsfJwYz-_g42bPTE1oA0zPJZGP81w67VPEzlEzlvTDBA4g0JVKQW_dOqqmcH3d9uvSdLpfB4H7ag7Y_H3lbXP-cXyxksEq_y0/file\n",
            "Resolving uca9ab5a88fbfca1a40dd4f28902.dl.dropboxusercontent.com (uca9ab5a88fbfca1a40dd4f28902.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6020:15::a27d:400f\n",
            "Connecting to uca9ab5a88fbfca1a40dd4f28902.dl.dropboxusercontent.com (uca9ab5a88fbfca1a40dd4f28902.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BJT0wlD5nOPsq7JSNdt2D6PeTl8Sk4WwDygUZHpVr5VypH3efTYBwzu4bI1xnhnNgteZAGIlKoYyA_z2GTspJXco4LQizCsv9i7w_CgVFynfMivV5iCYhQ81-hqiv93M3ncVVhKBjjErBc1N1pOfziMngSUNFPy2tegDe2QLirXQ3GsWFrbJrLrCgY_4X2xLAwJAEE2DJ_6XLAuAljHdDmt5jexz9-EGwt3OFuS5iSNa4WWaRpTITLmpQrtD4MyfsVLYn0IEhN2aknI8CAqAAEHUKQZ2vJ6_kZo-bJuTvGxwVdVV_akib5XiDSU7KnJ9Jze_ZunB8SGPz6Dwe68DJoPDHYM7qKm-u48evi-JLZQ5sA/file [following]\n",
            "--2021-02-20 04:40:13--  https://uca9ab5a88fbfca1a40dd4f28902.dl.dropboxusercontent.com/cd/0/inline2/BJT0wlD5nOPsq7JSNdt2D6PeTl8Sk4WwDygUZHpVr5VypH3efTYBwzu4bI1xnhnNgteZAGIlKoYyA_z2GTspJXco4LQizCsv9i7w_CgVFynfMivV5iCYhQ81-hqiv93M3ncVVhKBjjErBc1N1pOfziMngSUNFPy2tegDe2QLirXQ3GsWFrbJrLrCgY_4X2xLAwJAEE2DJ_6XLAuAljHdDmt5jexz9-EGwt3OFuS5iSNa4WWaRpTITLmpQrtD4MyfsVLYn0IEhN2aknI8CAqAAEHUKQZ2vJ6_kZo-bJuTvGxwVdVV_akib5XiDSU7KnJ9Jze_ZunB8SGPz6Dwe68DJoPDHYM7qKm-u48evi-JLZQ5sA/file\n",
            "Reusing existing connection to uca9ab5a88fbfca1a40dd4f28902.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 92415438 (88M) [application/zip]\n",
            "Saving to: ‘wiki.zip.1’\n",
            "\n",
            "wiki.zip.1          100%[===================>]  88.13M  22.9MB/s    in 4.4s    \n",
            "\n",
            "2021-02-20 04:40:19 (20.2 MB/s) - ‘wiki.zip.1’ saved [92415438/92415438]\n",
            "\n",
            "Archive:  wiki.zip\n",
            "replace __MACOSX/._wiki? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "Archive:  BEST-TrainingSet.zip\n",
            "replace __MACOSX/._BEST-TrainingSet? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0ALwvtzDZ-f"
      },
      "source": [
        "#Step 1: read the wikipedia text file\n",
        "with open(\"wiki/thwiki_chk.txt\") as f:\n",
        "    #the delimiter is one or more whitespace characters\n",
        "    input_text = re.compile(r\"\\s+\").split(f.read()) \n",
        "    #exclude an empty string from our input\n",
        "    input_text = [word for word in input_text if word != ''] "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXoFAfjaDcJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8172dbb7-49ed-4413-db13-f65b4d15fe09"
      },
      "source": [
        "tokens = input_text\n",
        "print(tokens[:10])\n",
        "print(\"total word count:\", len(tokens))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['หน้า', 'หลัก', 'วิกิพีเดีย', 'ดำเนินการ', 'โดย', 'มูลนิธิ', 'วิกิ', 'มีเดีย', 'องค์กร', 'ไม่']\n",
            "total word count: 36349066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDVMvTRci-Xu"
      },
      "source": [
        "# Step 2: Indexing (Assign a number to each word)\n",
        "\n",
        "The code below generates an indexed dataset(each word is represented by a number), a dictionary, a reversed dictionary\n",
        "\n",
        "## <font color='blue'>Homework Question 1:</font>\n",
        "<font color='white'>“UNK” is often used to represent an unknown word (a word which does not exist in your dictionary/training set). You can also represent a rare word with this token as well.  How do you define a rare word in your program? Explain in your own words and capture the screenshot of your code segment that is a part of this process</font>\n",
        "\n",
        " + <font color='white'>edit or replace create_index with your own code to set a threshold for rare words and replace them with \"UNK\"</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6NP7nQGi-Xw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "623af343-56e3-4288-83bf-e526b1d9805d"
      },
      "source": [
        "#step 2:Build dictionary and build a dataset(replace each word with its index)\n",
        "def create_index(input_text, min_thres_unk = 0, max_word_count = None):\n",
        "    # TODO#1 : edit or replace this function\n",
        "    words = [word for word in input_text ]\n",
        "    word_count = list()\n",
        "\n",
        "    #use set and len to get the number of unique words\n",
        "    word_count.extend(collections.Counter(words).most_common(len(set(words))))\n",
        "    #include a token for unknown word\n",
        "    word_count.append((\"UNK\",min_thres_unk+1))\n",
        "    #print out 10 most frequent words\n",
        "    print(word_count[:10])\n",
        "\n",
        "    dictionary = dict()\n",
        "    dictionary[\"for_keras_zero_padding\"] = 0\n",
        "    for word in word_count:\n",
        "        # Added here\n",
        "        if word[1] > min_thres_unk :\n",
        "            dictionary[word[0]] = len(dictionary)\n",
        "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    data = list()\n",
        "    for word in input_text:\n",
        "        # Add here\n",
        "        if word in dictionary :\n",
        "            data.append(dictionary[word])\n",
        "        else :\n",
        "            data.append(dictionary[\"UNK\"])\n",
        "\n",
        "    return data,dictionary, reverse_dictionary\n",
        "\n",
        "# call method with min_thres_unk=1ß\n",
        "dataset, dictionary, reverse_dictionary = create_index(tokens, 1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('ที่', 950006), ('ใน', 897329), ('เป็น', 726847), ('และ', 668116), ('การ', 619128), ('มี', 536738), ('ของ', 532237), ('ได้', 508117), (')', 359576), ('\"', 357830)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fotaYMgi-Xz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1737fa41-ce1e-47a6-82d5-eae679cc0463"
      },
      "source": [
        "print(\"size dataset: {}\".format(len(dataset)))\n",
        "print(\"size dictionary: {}\".format(len(dictionary)))\n",
        "print(\"output sample (dataset):\",dataset[:10])\n",
        "print(\"output sample (dictionary):\",{k: dictionary[k] for k in list(dictionary)[:10]})\n",
        "print(\"output sample (reverse dictionary):\",{k: reverse_dictionary[k] for k in list(reverse_dictionary)[:10]})"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size dataset: 36349066\n",
            "size dictionary: 295164\n",
            "output sample (dataset): [229, 208, 2453, 573, 15, 1829, 7149, 3124, 681, 24]\n",
            "output sample (dictionary): {'for_keras_zero_padding': 0, 'ที่': 1, 'ใน': 2, 'เป็น': 3, 'และ': 4, 'การ': 5, 'มี': 6, 'ของ': 7, 'ได้': 8, ')': 9}\n",
            "output sample (reverse dictionary): {0: 'for_keras_zero_padding', 1: 'ที่', 2: 'ใน', 3: 'เป็น', 4: 'และ', 5: 'การ', 6: 'มี', 7: 'ของ', 8: 'ได้', 9: ')'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1DDK9sQgBfT",
        "outputId": "16bf2375-3f2f-43f3-c692-056ccbe798e6"
      },
      "source": [
        "dataset.count(dictionary[\"UNK\"])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "406196"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HutTzPO7i-X3"
      },
      "source": [
        "# Step3: Create skip-grams (inputs for your model)\n",
        "Keras has a skipgrams-generator, the cell below shows us how it generates skipgrams \n",
        "\n",
        "## <font color='blue'>Homework Question 2:</font>\n",
        "<font color='white'>The negative samples are sampled from sampling_table.  Look through Keras source code to find out how they sample negative samples. Discuss the sampling technique taught in class and compare it to the Keras source code.</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwYFRO3YGryQ"
      },
      "source": [
        "<font color='red'>Q2: PUT YOUR ANSER HERE!!!</font>\r\n",
        "\r\n",
        "The probability function is different, for in-class it is ![Screenshot 2021-02-20 113025.png](data:image/png;base64,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) \r\n",
        "\r\n",
        "while keras it is `P(class) = (log(class + 2) - log(class + 1)) / log(range_max + 1)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C520WnI0i-X4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba7a4f45-64a6-4cdd-c6e8-faa7f2fa3339"
      },
      "source": [
        "# Step 3: Create data samples\n",
        "vocab_size = len(dictionary)\n",
        "skip_window = 1       # How many words to consider left and right.\n",
        "\n",
        "# TODO#2 check out keras source code and find out how their sampling technique works. Describe it in your own words.\n",
        "sample_set= dataset[:10]\n",
        "sampling_table = sequence.make_sampling_table(vocab_size)\n",
        "couples, labels = skipgrams(sample_set, vocab_size, window_size=skip_window, sampling_table=sampling_table)\n",
        "word_target, word_context = zip(*couples)\n",
        "word_target = np.array(word_target, dtype=\"int32\")\n",
        "word_context = np.array(word_context, dtype=\"int32\")\n",
        "\n",
        "print(couples, labels)\n",
        "\n",
        "for i in range(8):\n",
        "    print(reverse_dictionary[couples[i][0]],reverse_dictionary[couples[i][1]])\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[208, 83708], [208, 2453], [2453, 573], [208, 235515], [24, 681], [2453, 208], [3124, 145853], [2453, 115575], [24, 3408], [208, 229], [3124, 681], [3124, 7149], [3124, 285707], [2453, 219950]] [0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0]\n",
            "หลัก Merlin\n",
            "หลัก วิกิพีเดีย\n",
            "วิกิพีเดีย ดำเนินการ\n",
            "หลัก LaVoie\n",
            "ไม่ องค์กร\n",
            "วิกิพีเดีย หลัก\n",
            "มีเดีย -House\n",
            "วิกิพีเดีย (หงส์ทอง\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6UL0FhEi-X6"
      },
      "source": [
        "# Step 4: create the skip-gram model\n",
        "## <font color='blue'>Homework Question 3:</font>\n",
        " <font color='white'>Q3:  In your own words, discuss why Sigmoid is chosen as the activation function in the  skip-gram model.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oQLGkkuHG7o"
      },
      "source": [
        "<font color='red'>Q3: PUT YOUR ANSER HERE!!!</font>\r\n",
        "\r\n",
        "Since full softmax computation is expensive we use negative sampling with sigmoid which result in binary classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq7Eh9pXi-X7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59a59e64-1af4-460a-8aa0-d3cc8cf62c50"
      },
      "source": [
        "#reference: https://github.com/nzw0301/keras-examples/blob/master/Skip-gram-with-NS.ipynb\n",
        "dim_embedddings = 32\n",
        "V= len(dictionary)\n",
        "\n",
        "#step1: select the embedding of the target word from W\n",
        "w_inputs = Input(shape=(1, ), dtype='int32')\n",
        "w = Embedding(V+1, dim_embedddings)(w_inputs)\n",
        "\n",
        "#step2: select the embedding of the context word from C\n",
        "c_inputs = Input(shape=(1, ), dtype='int32')\n",
        "c  = Embedding(V+1, dim_embedddings)(c_inputs)\n",
        "\n",
        "#step3: compute the dot product:c_k*v_j\n",
        "o = Dot(axes=2)([w, c])\n",
        "o = Reshape((1,), input_shape=(1, 1))(o)\n",
        "\n",
        "#step4: normailize dot products into probability\n",
        "o = Activation('sigmoid')(o)\n",
        "#TO DO#4 Question: Why sigmoid?\n",
        "\n",
        "SkipGram = Model(inputs=[w_inputs, c_inputs], outputs=o)\n",
        "SkipGram.summary()\n",
        "opt=Adam(lr=0.01)\n",
        "SkipGram.compile(loss='binary_crossentropy', optimizer=opt)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 1, 32)        9445280     input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 1, 32)        9445280     input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 1, 1)         0           embedding_2[0][0]                \n",
            "                                                                 embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 1)            0           dot_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 1)            0           reshape_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 18,890,560\n",
            "Trainable params: 18,890,560\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImCXbXs-tvf0"
      },
      "source": [
        "precount = 5\r\n",
        "SkipGram.load_weights('/content/drive/MyDrive/Lecture/Senior/2/NLP/my_skipgram32_weights-hw-{}.h5'.format(precount-1))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Au6s6zZhCSX"
      },
      "source": [
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgR5p_h1i-X9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fd847ff-4431-42bd-95f5-4c23edba4c91"
      },
      "source": [
        "# you don't have to spend too much time training for your homework, you are allowed to do it on a smaller corpus\n",
        "# currently the dataset is 1/20 of the full text file.\n",
        "start = time.time()\n",
        "for c in range(5-precount):\n",
        "    prev_i=0\n",
        "    #it is likely that your GPU won't be able to handle large input\n",
        "    #just do it 100000 words at a time\n",
        "    for i in range(len(dataset)//100000):\n",
        "        #generate skipgrams\n",
        "        data, labels = skipgrams(sequence=dataset[prev_i*100000:(i*100000)+100000], vocabulary_size=V, window_size=2, negative_samples=4.)\n",
        "        x = [np.array(x) for x in zip(*data)]\n",
        "        y = np.array(labels, dtype=np.int32)\n",
        "        if x:\n",
        "            loss = SkipGram.train_on_batch(x, y)\n",
        "        prev_i = i \n",
        "        print(loss,i*100000)\n",
        "    end = time.time()\n",
        "    try :\n",
        "        print(\"Fin {} epoch, time used {}\".format(c+precount+1, end-start))\n",
        "        SkipGram.save_weights('/content/drive/MyDrive/Lecture/Senior/2/NLP/my_skipgram32_weights-hw-{}.h5'.format(c+precount))\n",
        "    except :\n",
        "        print(\"Shit\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.08427571505308151 0\n",
            "0.08742646127939224 100000\n",
            "0.08784645795822144 200000\n",
            "0.08215413987636566 300000\n",
            "0.08335798978805542 400000\n",
            "0.08644331246614456 500000\n",
            "0.08251987397670746 600000\n",
            "0.08288801461458206 700000\n",
            "0.08565189689397812 800000\n",
            "0.09073720127344131 900000\n",
            "0.09155973047018051 1000000\n",
            "0.08546909689903259 1100000\n",
            "0.0881616398692131 1200000\n",
            "0.08595794439315796 1300000\n",
            "0.08744120597839355 1400000\n",
            "0.09393469244241714 1500000\n",
            "0.09318114072084427 1600000\n",
            "0.09148932993412018 1700000\n",
            "0.09052248299121857 1800000\n",
            "0.09066732972860336 1900000\n",
            "0.09269395470619202 2000000\n",
            "0.08937197923660278 2100000\n",
            "0.08669354766607285 2200000\n",
            "0.08783873915672302 2300000\n",
            "0.08765890449285507 2400000\n",
            "0.08978867530822754 2500000\n",
            "0.08857735246419907 2600000\n",
            "0.09507140517234802 2700000\n",
            "0.10279669612646103 2800000\n",
            "0.10012421756982803 2900000\n",
            "0.09411836415529251 3000000\n",
            "0.09510136395692825 3100000\n",
            "0.10118669271469116 3200000\n",
            "0.10289372503757477 3300000\n",
            "0.1018512025475502 3400000\n",
            "0.09951899200677872 3500000\n",
            "0.09738079458475113 3600000\n",
            "0.09555411338806152 3700000\n",
            "0.09328344464302063 3800000\n",
            "0.09389892220497131 3900000\n",
            "0.09496302902698517 4000000\n",
            "0.09436823427677155 4100000\n",
            "0.09857945889234543 4200000\n",
            "0.10578428208827972 4300000\n",
            "0.0989980399608612 4400000\n",
            "0.09139607846736908 4500000\n",
            "0.10051114857196808 4600000\n",
            "0.10027711093425751 4700000\n",
            "0.09294038265943527 4800000\n",
            "0.0934852883219719 4900000\n",
            "0.09357959032058716 5000000\n",
            "0.0961054116487503 5100000\n",
            "0.09926435351371765 5200000\n",
            "0.09589582681655884 5300000\n",
            "0.0948433056473732 5400000\n",
            "0.09520428627729416 5500000\n",
            "0.09707006067037582 5600000\n",
            "0.09981272369623184 5700000\n",
            "0.09644339978694916 5800000\n",
            "0.09369070082902908 5900000\n",
            "0.09430922567844391 6000000\n",
            "0.09604641050100327 6100000\n",
            "0.09517253935337067 6200000\n",
            "0.09598521888256073 6300000\n",
            "0.0962110310792923 6400000\n",
            "0.09263180196285248 6500000\n",
            "0.0938912108540535 6600000\n",
            "0.09774185717105865 6700000\n",
            "0.09554149210453033 6800000\n",
            "0.09466224908828735 6900000\n",
            "0.09452558308839798 7000000\n",
            "0.0933036059141159 7100000\n",
            "0.0966174378991127 7200000\n",
            "0.09941185265779495 7300000\n",
            "0.09744430333375931 7400000\n",
            "0.09466727077960968 7500000\n",
            "0.09614825248718262 7600000\n",
            "0.09833141416311264 7700000\n",
            "0.09513457864522934 7800000\n",
            "0.09173700213432312 7900000\n",
            "0.09384795278310776 8000000\n",
            "0.09784498810768127 8100000\n",
            "0.0966336652636528 8200000\n",
            "0.09567841142416 8300000\n",
            "0.09751751273870468 8400000\n",
            "0.09831797331571579 8500000\n",
            "0.09788502007722855 8600000\n",
            "0.09628752619028091 8700000\n",
            "0.09706344455480576 8800000\n",
            "0.10014086216688156 8900000\n",
            "0.09852736443281174 9000000\n",
            "0.09412901103496552 9100000\n",
            "0.09554962068796158 9200000\n",
            "0.09857315570116043 9300000\n",
            "0.09704215079545975 9400000\n",
            "0.09893669933080673 9500000\n",
            "0.09902336448431015 9600000\n",
            "0.09360054135322571 9700000\n",
            "0.09639236330986023 9800000\n",
            "0.0953679010272026 9900000\n",
            "0.09307734668254852 10000000\n",
            "0.09918086975812912 10100000\n",
            "0.10211804509162903 10200000\n",
            "0.10424655675888062 10300000\n",
            "0.10277581214904785 10400000\n",
            "0.09875163435935974 10500000\n",
            "0.10219331085681915 10600000\n",
            "0.10374681651592255 10700000\n",
            "0.09857068955898285 10800000\n",
            "0.09772402793169022 10900000\n",
            "0.10010592639446259 11000000\n",
            "0.09695594757795334 11100000\n",
            "0.0944245457649231 11200000\n",
            "0.09493696689605713 11300000\n",
            "0.0948757603764534 11400000\n",
            "0.09394365549087524 11500000\n",
            "0.09327786415815353 11600000\n",
            "0.0927366092801094 11700000\n",
            "0.09458941966295242 11800000\n",
            "0.09978729486465454 11900000\n",
            "0.09888546913862228 12000000\n",
            "0.09688260406255722 12100000\n",
            "0.09562738239765167 12200000\n",
            "0.09444569796323776 12300000\n",
            "0.09847185015678406 12400000\n",
            "0.09940525144338608 12500000\n",
            "0.09711307287216187 12600000\n",
            "0.09616382420063019 12700000\n",
            "0.09854230284690857 12800000\n",
            "0.1006322056055069 12900000\n",
            "0.10057953745126724 13000000\n",
            "0.09962824732065201 13100000\n",
            "0.09718065708875656 13200000\n",
            "0.09651966392993927 13300000\n",
            "0.1001424714922905 13400000\n",
            "0.10303682088851929 13500000\n",
            "0.10071287304162979 13600000\n",
            "0.10131442546844482 13700000\n",
            "0.10437170416116714 13800000\n",
            "0.10287009924650192 13900000\n",
            "0.09959439188241959 14000000\n",
            "0.0977550595998764 14100000\n",
            "0.09807494282722473 14200000\n",
            "0.10193520784378052 14300000\n",
            "0.10303613543510437 14400000\n",
            "0.09836820513010025 14500000\n",
            "0.09914296865463257 14600000\n",
            "0.1002555787563324 14700000\n",
            "0.09701354056596756 14800000\n",
            "0.09556876868009567 14900000\n",
            "0.09505009651184082 15000000\n",
            "0.09888952225446701 15100000\n",
            "0.1022753193974495 15200000\n",
            "0.1007835790514946 15300000\n",
            "0.0992676168680191 15400000\n",
            "0.09819862991571426 15500000\n",
            "0.09713651984930038 15600000\n",
            "0.09694291651248932 15700000\n",
            "0.09898277372121811 15800000\n",
            "0.09999120235443115 15900000\n",
            "0.10140111297369003 16000000\n",
            "0.09972826391458511 16100000\n",
            "0.09647443145513535 16200000\n",
            "0.09425259381532669 16300000\n",
            "0.0963556244969368 16400000\n",
            "0.09897785633802414 16500000\n",
            "0.09906090795993805 16600000\n",
            "0.09888467192649841 16700000\n",
            "0.09632017463445663 16800000\n",
            "0.092128686606884 16900000\n",
            "0.09385962039232254 17000000\n",
            "0.09712831676006317 17100000\n",
            "0.09794536232948303 17200000\n",
            "0.09835262596607208 17300000\n",
            "0.10008963942527771 17400000\n",
            "0.1037256121635437 17500000\n",
            "0.10428128391504288 17600000\n",
            "0.10086484998464584 17700000\n",
            "0.09775421023368835 17800000\n",
            "0.09978717565536499 17900000\n",
            "0.09908630698919296 18000000\n",
            "0.098255455493927 18100000\n",
            "0.09674081951379776 18200000\n",
            "0.09257594496011734 18300000\n",
            "0.09504543244838715 18400000\n",
            "0.09717369079589844 18500000\n",
            "0.09442459791898727 18600000\n",
            "0.09624025970697403 18700000\n",
            "0.09663157165050507 18800000\n",
            "0.09551223367452621 18900000\n",
            "0.0967942476272583 19000000\n",
            "0.1003664955496788 19100000\n",
            "0.10061295330524445 19200000\n",
            "0.09767842292785645 19300000\n",
            "0.09696261584758759 19400000\n",
            "0.09526316821575165 19500000\n",
            "0.09380945563316345 19600000\n",
            "0.09519113600254059 19700000\n",
            "0.09473258256912231 19800000\n",
            "0.0980144664645195 19900000\n",
            "0.09984184801578522 20000000\n",
            "0.09673616290092468 20100000\n",
            "0.09957122057676315 20200000\n",
            "0.09893452376127243 20300000\n",
            "0.09778482466936111 20400000\n",
            "0.09836218506097794 20500000\n",
            "0.09750743210315704 20600000\n",
            "0.0987686961889267 20700000\n",
            "0.09890910238027573 20800000\n",
            "0.09583776444196701 20900000\n",
            "0.09458325058221817 21000000\n",
            "0.09134096652269363 21100000\n",
            "0.08923080563545227 21200000\n",
            "0.09214010089635849 21300000\n",
            "0.09192974865436554 21400000\n",
            "0.0899450033903122 21500000\n",
            "0.0931185856461525 21600000\n",
            "0.09247726202011108 21700000\n",
            "0.09052618592977524 21800000\n",
            "0.09242038428783417 21900000\n",
            "0.08331922441720963 22000000\n",
            "0.08030131459236145 22100000\n",
            "0.09216843545436859 22200000\n",
            "0.0988701581954956 22300000\n",
            "0.09668730199337006 22400000\n",
            "0.09450622648000717 22500000\n",
            "0.09300990402698517 22600000\n",
            "0.09054941684007645 22700000\n",
            "0.09111877530813217 22800000\n",
            "0.09376787394285202 22900000\n",
            "0.09770460426807404 23000000\n",
            "0.09599915891885757 23100000\n",
            "0.09346230328083038 23200000\n",
            "0.09493494033813477 23300000\n",
            "0.09346012771129608 23400000\n",
            "0.09353306889533997 23500000\n",
            "0.09625445306301117 23600000\n",
            "0.09233629703521729 23700000\n",
            "0.08961579948663712 23800000\n",
            "0.0911080539226532 23900000\n",
            "0.09393775463104248 24000000\n",
            "0.09278909116983414 24100000\n",
            "0.09139518439769745 24200000\n",
            "0.0917421206831932 24300000\n",
            "0.0910763069987297 24400000\n",
            "0.08916204422712326 24500000\n",
            "0.08832849562168121 24600000\n",
            "0.09126970171928406 24700000\n",
            "0.0938953086733818 24800000\n",
            "0.09491997957229614 24900000\n",
            "0.09320612996816635 25000000\n",
            "0.09219025075435638 25100000\n",
            "0.09217426925897598 25200000\n",
            "0.09237305074930191 25300000\n",
            "0.092787966132164 25400000\n",
            "0.09336178004741669 25500000\n",
            "0.09350654482841492 25600000\n",
            "0.09248919039964676 25700000\n",
            "0.09069675952196121 25800000\n",
            "0.09182567894458771 25900000\n",
            "0.0917610377073288 26000000\n",
            "0.09300880134105682 26100000\n",
            "0.09649664908647537 26200000\n",
            "0.09759094566106796 26300000\n",
            "0.09808601438999176 26400000\n",
            "0.09705139696598053 26500000\n",
            "0.09514209628105164 26600000\n",
            "0.09294943511486053 26700000\n",
            "0.09324067085981369 26800000\n",
            "0.0932542011141777 26900000\n",
            "0.09060533344745636 27000000\n",
            "0.09173103421926498 27100000\n",
            "0.09395145624876022 27200000\n",
            "0.09034106880426407 27300000\n",
            "0.09040932357311249 27400000\n",
            "0.09514548629522324 27500000\n",
            "0.0905197486281395 27600000\n",
            "0.08905625343322754 27700000\n",
            "0.09518101811408997 27800000\n",
            "0.09725046157836914 27900000\n",
            "0.09550727903842926 28000000\n",
            "0.09229715168476105 28100000\n",
            "0.09188219904899597 28200000\n",
            "0.0904717817902565 28300000\n",
            "0.09014452993869781 28400000\n",
            "0.09160017222166061 28500000\n",
            "0.09160090237855911 28600000\n",
            "0.09267948567867279 28700000\n",
            "0.09552852064371109 28800000\n",
            "0.09959270805120468 28900000\n",
            "0.09717398881912231 29000000\n",
            "0.09227198362350464 29100000\n",
            "0.0956878736615181 29200000\n",
            "0.09770263731479645 29300000\n",
            "0.09676159918308258 29400000\n",
            "0.095365509390831 29500000\n",
            "0.09321902692317963 29600000\n",
            "0.0979553759098053 29700000\n",
            "0.10119564831256866 29800000\n",
            "0.09721437841653824 29900000\n",
            "0.09875314682722092 30000000\n",
            "0.09932056814432144 30100000\n",
            "0.09571631252765656 30200000\n",
            "0.09747925400733948 30300000\n",
            "0.09936635196208954 30400000\n",
            "0.09536099433898926 30500000\n",
            "0.09155203402042389 30600000\n",
            "0.09360989183187485 30700000\n",
            "0.0953657403588295 30800000\n",
            "0.0956767126917839 30900000\n",
            "0.0949307456612587 31000000\n",
            "0.09304066747426987 31100000\n",
            "0.09338295459747314 31200000\n",
            "0.09695655852556229 31300000\n",
            "0.09554846584796906 31400000\n",
            "0.09446756541728973 31500000\n",
            "0.0988936722278595 31600000\n",
            "0.0984933152794838 31700000\n",
            "0.0991431325674057 31800000\n",
            "0.09704360365867615 31900000\n",
            "0.09464800357818604 32000000\n",
            "0.09372681379318237 32100000\n",
            "0.09596975892782211 32200000\n",
            "0.09820356220006943 32300000\n",
            "0.093233123421669 32400000\n",
            "0.09476378560066223 32500000\n",
            "0.09629824012517929 32600000\n",
            "0.09059727936983109 32700000\n",
            "0.0921841710805893 32800000\n",
            "0.0943378359079361 32900000\n",
            "0.09000039845705032 33000000\n",
            "0.08975177258253098 33100000\n",
            "0.0898093581199646 33200000\n",
            "0.0920775830745697 33300000\n",
            "0.0939508005976677 33400000\n",
            "0.09334246069192886 33500000\n",
            "0.0895206481218338 33600000\n",
            "0.08631482720375061 33700000\n",
            "0.09085601568222046 33800000\n",
            "0.09582838416099548 33900000\n",
            "0.09573838114738464 34000000\n",
            "0.09546436369419098 34100000\n",
            "0.0962437316775322 34200000\n",
            "0.0978996530175209 34300000\n",
            "0.09824221581220627 34400000\n",
            "0.09497716277837753 34500000\n",
            "0.08946867287158966 34600000\n",
            "0.08923348784446716 34700000\n",
            "0.09040758013725281 34800000\n",
            "0.08913683891296387 34900000\n",
            "0.0896664410829544 35000000\n",
            "0.08960223197937012 35100000\n",
            "0.0897781252861023 35200000\n",
            "0.09119895100593567 35300000\n",
            "0.09253805130720139 35400000\n",
            "0.09685492515563965 35500000\n",
            "0.10113725066184998 35600000\n",
            "0.0921100452542305 35700000\n",
            "0.0862286165356636 35800000\n",
            "0.09029309451580048 35900000\n",
            "0.09145812690258026 36000000\n",
            "0.09017986804246902 36100000\n",
            "0.09481474757194519 36200000\n",
            "Fin 4 epoch, time used 11695.4242374897\n",
            "0.08044284582138062 0\n",
            "0.08362855762243271 100000\n",
            "0.0842166468501091 200000\n",
            "0.07922249287366867 300000\n",
            "0.08006785809993744 400000\n",
            "0.08305802941322327 500000\n",
            "0.07957067340612411 600000\n",
            "0.07991603016853333 700000\n",
            "0.08243674039840698 800000\n",
            "0.08699465543031693 900000\n",
            "0.08787213265895844 1000000\n",
            "0.08165404200553894 1100000\n",
            "0.0842478945851326 1200000\n",
            "0.08249445259571075 1300000\n",
            "0.08440975844860077 1400000\n",
            "0.09008562564849854 1500000\n",
            "0.08919236063957214 1600000\n",
            "0.08803094923496246 1700000\n",
            "0.087029829621315 1800000\n",
            "0.08702994883060455 1900000\n",
            "0.0889233723282814 2000000\n",
            "0.08580616861581802 2100000\n",
            "0.0834159329533577 2200000\n",
            "0.08491358160972595 2300000\n",
            "0.08471548557281494 2400000\n",
            "0.08627254515886307 2500000\n",
            "0.08539046347141266 2600000\n",
            "0.091190867125988 2700000\n",
            "0.09862357378005981 2800000\n",
            "0.09632794559001923 2900000\n",
            "0.09057533740997314 3000000\n",
            "0.09138200432062149 3100000\n",
            "0.09690471738576889 3200000\n",
            "0.09832180291414261 3300000\n",
            "0.09764997661113739 3400000\n",
            "0.09569469839334488 3500000\n",
            "0.0937570184469223 3600000\n",
            "0.09165120869874954 3700000\n",
            "0.08965001255273819 3800000\n",
            "0.09000179916620255 3900000\n",
            "0.09123353660106659 4000000\n",
            "0.09062164276838303 4100000\n",
            "0.09461662918329239 4200000\n",
            "0.10120588541030884 4300000\n",
            "0.094872385263443 4400000\n",
            "0.08768846094608307 4500000\n",
            "0.0967576801776886 4600000\n",
            "0.09644117951393127 4700000\n",
            "0.08948764950037003 4800000\n",
            "0.08980945497751236 4900000\n",
            "0.0902298167347908 5000000\n",
            "0.0929650142788887 5100000\n",
            "0.09544742852449417 5200000\n",
            "0.0921662226319313 5300000\n",
            "0.0911073386669159 5400000\n",
            "0.09106742590665817 5500000\n",
            "0.09337738156318665 5600000\n",
            "0.09561053663492203 5700000\n",
            "0.09243930131196976 5800000\n",
            "0.08998648822307587 5900000\n",
            "0.09036092460155487 6000000\n",
            "0.0921301394701004 6100000\n",
            "0.09138414263725281 6200000\n",
            "0.0918230190873146 6300000\n",
            "0.09211061149835587 6400000\n",
            "0.08906129002571106 6500000\n",
            "0.0901743695139885 6600000\n",
            "0.09370235353708267 6700000\n",
            "0.09184955060482025 6800000\n",
            "0.09060263633728027 6900000\n",
            "0.0905362144112587 7000000\n",
            "0.08990567177534103 7100000\n",
            "0.09287665039300919 7200000\n",
            "0.09514692425727844 7300000\n",
            "0.09350527822971344 7400000\n",
            "0.09075671434402466 7500000\n",
            "0.09242905676364899 7600000\n",
            "0.09420671314001083 7700000\n",
            "0.09124793112277985 7800000\n",
            "0.08825661242008209 7900000\n",
            "0.09035024791955948 8000000\n",
            "0.0942596048116684 8100000\n",
            "0.09312207996845245 8200000\n",
            "0.09190230816602707 8300000\n",
            "0.09366674721240997 8400000\n",
            "0.09469801187515259 8500000\n",
            "0.09421471506357193 8600000\n",
            "0.09267541021108627 8700000\n",
            "0.09301083534955978 8800000\n",
            "0.09590038657188416 8900000\n",
            "0.09427831321954727 9000000\n",
            "0.09011752903461456 9100000\n",
            "0.09142004698514938 9200000\n",
            "0.09463729709386826 9300000\n",
            "0.09324445575475693 9400000\n",
            "0.09514527022838593 9500000\n",
            "0.095132015645504 9600000\n",
            "0.09008488804101944 9700000\n",
            "0.09271176904439926 9800000\n",
            "0.09146824479103088 9900000\n",
            "0.08975658565759659 10000000\n",
            "0.095750592648983 10100000\n",
            "0.09786819666624069 10200000\n",
            "0.09980102628469467 10300000\n",
            "0.09822053462266922 10400000\n",
            "0.094467893242836 10500000\n",
            "0.09788607805967331 10600000\n",
            "0.09932548552751541 10700000\n",
            "0.09459105879068375 10800000\n",
            "0.09371117502450943 10900000\n",
            "0.09642834216356277 11000000\n",
            "0.09367311745882034 11100000\n",
            "0.0908164232969284 11200000\n",
            "0.09117582440376282 11300000\n",
            "0.09125640243291855 11400000\n",
            "0.09056978672742844 11500000\n",
            "0.08994770795106888 11600000\n",
            "0.08922268450260162 11700000\n",
            "0.09124590456485748 11800000\n",
            "0.09591161459684372 11900000\n",
            "0.0949494019150734 12000000\n",
            "0.09281650930643082 12100000\n",
            "0.09185759723186493 12200000\n",
            "0.09047490358352661 12300000\n",
            "0.0945071429014206 12400000\n",
            "0.09536324441432953 12500000\n",
            "0.0930505320429802 12600000\n",
            "0.09240612387657166 12700000\n",
            "0.09442909061908722 12800000\n",
            "0.09647426009178162 12900000\n",
            "0.09617388993501663 13000000\n",
            "0.09584009647369385 13100000\n",
            "0.09342104941606522 13200000\n",
            "0.09283574670553207 13300000\n",
            "0.09635403752326965 13400000\n",
            "0.09866554290056229 13500000\n",
            "0.09655030071735382 13600000\n",
            "0.09709011763334274 13700000\n",
            "0.09971746802330017 13800000\n",
            "0.09804082661867142 13900000\n",
            "0.09530087560415268 14000000\n",
            "0.0932219997048378 14100000\n",
            "0.0938536673784256 14200000\n",
            "0.09769552201032639 14300000\n",
            "0.09874141216278076 14400000\n",
            "0.09407950937747955 14500000\n",
            "0.09338919818401337 14600000\n",
            "0.09488815069198608 14700000\n",
            "0.09334129840135574 14800000\n",
            "0.09183643013238907 14900000\n",
            "0.09069190919399261 15000000\n",
            "0.09433989226818085 15100000\n",
            "0.09787414222955704 15200000\n",
            "0.09651083499193192 15300000\n",
            "0.09519448131322861 15400000\n",
            "0.09438593685626984 15500000\n",
            "0.09340184181928635 15600000\n",
            "0.09329479187726974 15700000\n",
            "0.09491937607526779 15800000\n",
            "0.0961635559797287 15900000\n",
            "0.09748813509941101 16000000\n",
            "0.09593265503644943 16100000\n",
            "0.09275946021080017 16200000\n",
            "0.0908287763595581 16300000\n",
            "0.09244413673877716 16400000\n",
            "0.09451127052307129 16500000\n",
            "0.09509613364934921 16600000\n",
            "0.09504127502441406 16700000\n",
            "0.09242315590381622 16800000\n",
            "0.0886610820889473 16900000\n",
            "0.09029781073331833 17000000\n",
            "0.09308208525180817 17100000\n",
            "0.09420955181121826 17200000\n",
            "0.09440869092941284 17300000\n",
            "0.09590205550193787 17400000\n",
            "0.09898919612169266 17500000\n",
            "0.09969757497310638 17600000\n",
            "0.09692330658435822 17700000\n",
            "0.09395097196102142 17800000\n",
            "0.0955534502863884 17900000\n",
            "0.09509952366352081 18000000\n",
            "0.09401912242174149 18100000\n",
            "0.0927594006061554 18200000\n",
            "0.08865378797054291 18300000\n",
            "0.09115710109472275 18400000\n",
            "0.09303540736436844 18500000\n",
            "0.09052346646785736 18600000\n",
            "0.0923161432147026 18700000\n",
            "0.09287766367197037 18800000\n",
            "0.09162579476833344 18900000\n",
            "0.09296515583992004 19000000\n",
            "0.09622041136026382 19100000\n",
            "0.0964738130569458 19200000\n",
            "0.09386374056339264 19300000\n",
            "0.09342359751462936 19400000\n",
            "0.09195805341005325 19500000\n",
            "0.09007212519645691 19600000\n",
            "0.09172182530164719 19700000\n",
            "0.09134004265069962 19800000\n",
            "0.09348629415035248 19900000\n",
            "0.09514009207487106 20000000\n",
            "0.09287898987531662 20100000\n",
            "0.09566370397806168 20200000\n",
            "0.09481711685657501 20300000\n",
            "0.09375407546758652 20400000\n",
            "0.09434976428747177 20500000\n",
            "0.09351909905672073 20600000\n",
            "0.0950586125254631 20700000\n",
            "0.0948382094502449 20800000\n",
            "0.09219773858785629 20900000\n",
            "0.0905647799372673 21000000\n",
            "0.08743151277303696 21100000\n",
            "0.08533470332622528 21200000\n",
            "0.08810184895992279 21300000\n",
            "0.08791287988424301 21400000\n",
            "0.08573156595230103 21500000\n",
            "0.08915594965219498 21600000\n",
            "0.08879479020833969 21700000\n",
            "0.08631271123886108 21800000\n",
            "0.08805667608976364 21900000\n",
            "0.07885515689849854 22000000\n",
            "0.07567901909351349 22100000\n",
            "0.0880090519785881 22200000\n",
            "0.09488863497972488 22300000\n",
            "0.09290912002325058 22400000\n",
            "0.09121212363243103 22500000\n",
            "0.089152492582798 22600000\n",
            "0.08664491027593613 22700000\n",
            "0.08711820095777512 22800000\n",
            "0.08965528756380081 22900000\n",
            "0.09306342899799347 23000000\n",
            "0.09196639060974121 23100000\n",
            "0.08961807191371918 23200000\n",
            "0.091151162981987 23300000\n",
            "0.08977428823709488 23400000\n",
            "0.09003258496522903 23500000\n",
            "0.09273010492324829 23600000\n",
            "0.08877330273389816 23700000\n",
            "0.08580002933740616 23800000\n",
            "0.08746804296970367 23900000\n",
            "0.09029705822467804 24000000\n",
            "0.08928737044334412 24100000\n",
            "0.08807642757892609 24200000\n",
            "0.0883200615644455 24300000\n",
            "0.08739592134952545 24400000\n",
            "0.08573600649833679 24500000\n",
            "0.08507147431373596 24600000\n",
            "0.08742472529411316 24700000\n",
            "0.09018857032060623 24800000\n",
            "0.09109137207269669 24900000\n",
            "0.08975697308778763 25000000\n",
            "0.08899879455566406 25100000\n",
            "0.08909012377262115 25200000\n",
            "0.08898580819368362 25300000\n",
            "0.08942573517560959 25400000\n",
            "0.08964670449495316 25500000\n",
            "0.08974573761224747 25600000\n",
            "0.08871942013502121 25700000\n",
            "0.087185800075531 25800000\n",
            "0.08852275460958481 25900000\n",
            "0.08809927105903625 26000000\n",
            "0.08975013345479965 26100000\n",
            "0.0927441418170929 26200000\n",
            "0.09307302534580231 26300000\n",
            "0.09370702505111694 26400000\n",
            "0.09344988316297531 26500000\n",
            "0.09148714691400528 26600000\n",
            "0.08909827470779419 26700000\n",
            "0.08928559720516205 26800000\n",
            "0.08970648050308228 26900000\n",
            "0.08711938560009003 27000000\n",
            "0.08825983852148056 27100000\n",
            "0.09021027386188507 27200000\n",
            "0.08715395629405975 27300000\n",
            "0.08676421642303467 27400000\n",
            "0.09079930186271667 27500000\n",
            "0.08657053858041763 27600000\n",
            "0.08538353443145752 27700000\n",
            "0.09106093645095825 27800000\n",
            "0.09300114214420319 27900000\n",
            "0.09164763987064362 28000000\n",
            "0.08861997723579407 28100000\n",
            "0.08838877081871033 28200000\n",
            "0.08702060580253601 28300000\n",
            "0.08698006719350815 28400000\n",
            "0.0880596935749054 28500000\n",
            "0.08781790733337402 28600000\n",
            "0.0890439972281456 28700000\n",
            "0.09183650463819504 28800000\n",
            "0.09539729356765747 28900000\n",
            "0.09317288547754288 29000000\n",
            "0.08841009438037872 29100000\n",
            "0.09189123660326004 29200000\n",
            "0.09350911527872086 29300000\n",
            "0.09251244366168976 29400000\n",
            "0.09137012809515 29500000\n",
            "0.08926665037870407 29600000\n",
            "0.09334298968315125 29700000\n",
            "0.09652836620807648 29800000\n",
            "0.09309537708759308 29900000\n",
            "0.0942782610654831 30000000\n",
            "0.09456910192966461 30100000\n",
            "0.09161539375782013 30200000\n",
            "0.09276095777750015 30300000\n",
            "0.09485431760549545 30400000\n",
            "0.09158530831336975 30500000\n",
            "0.08771953731775284 30600000\n",
            "0.08970426768064499 30700000\n",
            "0.0914870947599411 30800000\n",
            "0.09171187877655029 30900000\n",
            "0.09112925827503204 31000000\n",
            "0.08916951715946198 31100000\n",
            "0.08953867107629776 31200000\n",
            "0.09298992902040482 31300000\n",
            "0.09125836938619614 31400000\n",
            "0.09074299037456512 31500000\n",
            "0.09496291726827621 31600000\n",
            "0.0946100503206253 31700000\n",
            "0.09522368013858795 31800000\n",
            "0.09289317578077316 31900000\n",
            "0.09084958583116531 32000000\n",
            "0.09010784327983856 32100000\n",
            "0.09161127358675003 32200000\n",
            "0.0937911793589592 32300000\n",
            "0.08914011716842651 32400000\n",
            "0.09072208404541016 32500000\n",
            "0.09199679642915726 32600000\n",
            "0.08622507750988007 32700000\n",
            "0.08816976100206375 32800000\n",
            "0.09016405045986176 32900000\n",
            "0.08627597242593765 33000000\n",
            "0.08619546890258789 33100000\n",
            "0.0863988995552063 33200000\n",
            "0.08810825645923615 33300000\n",
            "0.089936263859272 33400000\n",
            "0.08924219757318497 33500000\n",
            "0.08553268760442734 33600000\n",
            "0.0826018750667572 33700000\n",
            "0.08645658940076828 33800000\n",
            "0.09097310155630112 33900000\n",
            "0.09090431779623032 34000000\n",
            "0.0911925733089447 34100000\n",
            "0.09168042987585068 34200000\n",
            "0.0928492322564125 34300000\n",
            "0.09315717220306396 34400000\n",
            "0.09039487689733505 34500000\n",
            "0.08501123636960983 34600000\n",
            "0.08391329646110535 34700000\n",
            "0.0848417580127716 34800000\n",
            "0.08463039249181747 34900000\n",
            "0.08540745824575424 35000000\n",
            "0.0857827365398407 35100000\n",
            "0.08545336872339249 35200000\n",
            "0.08687685430049896 35300000\n",
            "0.08736700564622879 35400000\n",
            "0.09088974446058273 35500000\n",
            "0.09513076394796371 35600000\n",
            "0.08668176084756851 35700000\n",
            "0.08083866536617279 35800000\n",
            "0.08446786552667618 35900000\n",
            "0.0860181376338005 36000000\n",
            "0.08442198485136032 36100000\n",
            "0.08977460861206055 36200000\n",
            "Fin 5 epoch, time used 23389.155388832092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7UD13eKki-YA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cf56d14-2348-487e-ceb8-fdac9fefd7fa"
      },
      "source": [
        "#Get weight of the embedding layer\n",
        "final_embeddings=SkipGram.get_weights()[0]\n",
        "print(final_embeddings)\n",
        "print(final_embeddings.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-4.4611838e-02  3.7928227e-02  1.3160955e-02 ...  2.9036392e-02\n",
            "  -2.6165187e-02 -3.4383155e-02]\n",
            " [ 1.0817220e+00 -3.7302560e-01  2.0129755e-01 ... -6.4706260e-01\n",
            "   1.6708435e+00  2.1098122e-01]\n",
            " [ 1.1989682e+00  8.8132089e-01 -6.1816007e-01 ...  9.6960938e-01\n",
            "   1.0882146e+00 -2.6347410e-02]\n",
            " ...\n",
            " [ 4.2698529e-02 -4.0585399e-03 -4.5627501e-02 ... -3.1900242e-02\n",
            "   1.5793290e-02  5.4097176e-04]\n",
            " [ 4.1859601e-02  4.0867340e-02 -5.8749318e-03 ...  3.7457202e-02\n",
            "   3.9320674e-02 -1.1523254e-03]\n",
            " [-1.4914992e-01 -1.1878514e-01 -2.6940602e-01 ... -4.8992574e-01\n",
            "   8.8861096e-01  5.2244741e-01]]\n",
            "(295165, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ovPmh6Ri-YC"
      },
      "source": [
        "# Step 5: Intrinsic Evaluation: Word Vector Analogies\n",
        "## <font color='blue'>Homework Question 4: </font>\n",
        "<font color='white'> Read section 2.1 and 2.3 in this [lecture note](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes02-wordvecs2.pdf). Come up with 10 semantic analogy examples and report results produced by your word embeddings </font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8rTxYaLi-YD"
      },
      "source": [
        "# TODO#4:Come up with 10 semantic analogy examples and report results produced by your word embeddings \n",
        "#and tell us what you observe "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlDMnA45OvG3"
      },
      "source": [
        "From the nearest function, it shows that the model grouping the word accurately but when it comes to analogy the result is not good enough.\r\n",
        "For instance, when it guess \"พระราชา\" : \"ชาย\" :: \"พระราชินี\" the expected result is \"หญิง\" but the model return \"ชาย\" as the nearest follow by prefer result \"หญิง\", this may caused by poor tokenizing, small number of epoch or context from the given data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V36hpJu1NbR"
      },
      "source": [
        "def word2vec(word):\r\n",
        "    try :\r\n",
        "        return final_embeddings[dictionary[word]]\r\n",
        "    except:\r\n",
        "        print(\"UNK\", word)\r\n",
        "        return final_embeddings[dictionary[\"UNK\"]]\r\n",
        "\r\n",
        "def nearest(word):\r\n",
        "    vec = word2vec(word)\r\n",
        "    cosine_sim = np.dot(vec, final_embeddings.T) / (np.linalg.norm(vec)*np.linalg.norm(final_embeddings, axis=1))\r\n",
        "    indexes = np.argpartition(cosine_sim, -10)[-10:]\r\n",
        "    return np.array([reverse_dictionary[e] for e in indexes])\r\n",
        "\r\n",
        "def analogy(a, b, c):\r\n",
        "    a_vec = word2vec(a)\r\n",
        "    b_vec = word2vec(b)\r\n",
        "    c_vec = word2vec(c)\r\n",
        "    d_vec = b_vec - a_vec + c_vec\r\n",
        "    cosine_sim = np.dot(d_vec, final_embeddings.T) / (np.linalg.norm(d_vec)*np.linalg.norm(final_embeddings, axis=1))\r\n",
        "    indexes = np.argsort(cosine_sim)[-10:][::-1]\r\n",
        "    print(\"{} : {} :: {} : {}\".format(a, b, c, [reverse_dictionary[d] for d in indexes]))"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3JnAYYCID8V",
        "outputId": "736ffcda-791a-4fcb-a385-b8c80668cafc"
      },
      "source": [
        "nearest(\"แดง\")"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['เทา', 'ม่วง', 'สี', 'เขียว', 'น้ำเงิน', 'เหลือง', 'ดำ', 'ชมพู',\n",
              "       'แดง', 'ขาว'], dtype='<U7')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4a558TAO7wd",
        "outputId": "6744dc90-cd89-4c3a-cb78-cd95eebd6ce6"
      },
      "source": [
        "nearest(\"มกราคม\")"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['พฤษภาคม', 'กันยายน', 'มิถุนายน', 'สิงหาคม', 'มีนาคม', 'เมษายน',\n",
              "       'กุมภาพันธ์', 'มกราคม', 'ตุลาคม', 'พฤศจิกายน'], dtype='<U10')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdpeJB6OPCm6",
        "outputId": "87754b10-97f9-4196-90f5-d1df0dd33556"
      },
      "source": [
        "nearest(\"ญี่ปุ่น\")"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['อิตาลี', 'ไต้หวัน', 'จีน', 'ของจีน', 'สากล', 'อังกฤษ', 'เยอรมัน',\n",
              "       'ดัตช์', 'เกาหลี', 'ญี่ปุ่น'], dtype='<U7')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWIscrWW4DET",
        "outputId": "2edb0763-7271-46b8-e09e-397cce778fe1"
      },
      "source": [
        "analogy(\"พระราชา\", \"ชาย\" , \"พระราชินี\")\r\n",
        "for a in [\"ลาว\", \"สิงคโปร์\", \"พม่า\", \"จีน\", \"เขมร\"] :\r\n",
        "    analogy(\"ญี่ปุ่น\", \"โตเกียว\" , a)\r\n",
        "    analogy(\"ไทย\", \"กรุงเทพ\" , a)\r\n"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "พระราชา : ชาย :: พระราชินี : ['ชาย', 'หญิง', 'กิจวิจารณ์', 'หลุย', 'เวลส์', 'บันดาร์เซอรี', 'เอี่ยมศรี', 'วอลเลย์บอล', 'อัครินทร์', 'คฤพล']\n",
            "ญี่ปุ่น : โตเกียว :: ลาว : ['สุราษฎร์ธานี', 'นครศรีธรรมราช', 'พังงา', 'ภูเก็ต', 'หนองบัว', 'นคร', 'ซึ่งตั้ง', 'ตราด', 'ย่าน', 'ยะลา']\n",
            "ไทย : กรุงเทพ :: ลาว : ['ซึ่งตั้ง', 'สนามบิน', 'ลาดพร้าว', 'ปทุมวัน', 'นครศรีธรรมราช', 'ตั้ง', 'ถนนพระราม', 'นนทบุรี', 'ทวีวัฒนา', 'ตราด']\n",
            "ญี่ปุ่น : โตเกียว :: สิงคโปร์ : ['โตเกียว', 'ชานเมือง', 'สุราษฎร์ธานี', 'ประเทศออสเตรเลีย', 'พลาซา', 'เซนท์', 'ลาดพร้าว', 'บางนา', 'สนามบิน', 'ตราด']\n",
            "ไทย : กรุงเทพ :: สิงคโปร์ : ['สนามบิน', 'รัฐอิลลินอยส์', 'ลอนดอน', 'รัฐนิวยอร์ก', 'ลาดพร้าว', 'ซึ่งตั้ง', 'ไอวีบี', 'พลาซา', 'ประเทศสหรัฐอเมริกา', 'โตเกียว']\n",
            "ญี่ปุ่น : โตเกียว :: พม่า : ['นคร', 'อาณาเขต', 'บรรจบ', 'ยะลา', 'ประเทศเวียดนาม', 'ตราด', 'ปัตตานี', 'นครศรีธรรมราช', 'เลียบ', 'เสียมราฐ']\n",
            "ไทย : กรุงเทพ :: พม่า : ['ย่างกุ้ง', 'มุ่งหน้า', 'ห่าง', 'สนามบิน', 'ปริมณฑล', 'ซึ่งตั้ง', 'เลี้ยว', 'ไอบีเรีย', 'ถิ่นฐาน', 'อาณาเขต']\n",
            "ญี่ปุ่น : โตเกียว :: จีน : ['ภูเก็ต', 'ย่าน', 'สุราษฎร์ธานี', 'โตเกียว', 'ซึ่งตั้ง', 'ยะลา', 'ปักกิ่ง', 'พังงา', 'ชานเมือง', 'แม่ฮ่องสอน']\n",
            "ไทย : กรุงเทพ :: จีน : ['ซึ่งตั้ง', 'กลาง', 'ไกโด', 'สนามบิน', 'พิทยากร', 'เขาแอลป์', 'ลาดพร้าว', 'ญี่ปุ่นตะวัน', 'ประเทศเยอรมันตะวัน', 'เทือก']\n",
            "ญี่ปุ่น : โตเกียว :: เขมร : ['นคร', 'สุราษฎร์ธานี', 'นครศรีธรรมราช', 'กำแพงเพชร', 'เพชรบูรณ์', 'หนองบัว', 'เชียงราย', 'แม่ฮ่องสอน', 'ภูเก็ต', 'ยะลา']\n",
            "ไทย : กรุงเทพ :: เขมร : ['ซึ่งตั้ง', 'ปทุมวัน', 'นนทบุรี', 'ประสานมิตร', 'จังหวัดนครปฐม', 'นครศรีธรรมราช', 'เพชรบูรณ์', 'ตั้ง', 'สนามบิน', 'นครราชสีมา']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLqG8WaNi-YE"
      },
      "source": [
        "# Step 6: Extrinsic Evaluation\n",
        "\n",
        "## <font color='blue'>Homework Question5:</font>\n",
        "<font color='white'>\n",
        "Use the word embeddings from the skip-gram model as pre-trained weights in a classification model. Compare the result the with the same classification model that does not use the pre-trained weights. \n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBPutcxEi-YF"
      },
      "source": [
        "all_news_filepath = glob.glob('BEST-TrainingSet/news/*.txt')\n",
        "all_novel_filepath = glob.glob('BEST-TrainingSet/novel/*.txt')\n",
        "all_article_filepath = glob.glob('BEST-TrainingSet/article/*.txt')\n",
        "all_encyclopedia_filepath = glob.glob('BEST-TrainingSet/encyclopedia/*.txt')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaX-L5n4i-YG"
      },
      "source": [
        "#preparing data for the classificaiton model\n",
        "#In your homework, we will only use the first 2000 words in each text file\n",
        "#any text file that has less than 2000 words will be padded\n",
        "#reason:just to make this homework feasible under limited time and resource\n",
        "max_length = 2000\n",
        "def word_to_index(word):\n",
        "    if word in dictionary:\n",
        "        return dictionary[word]\n",
        "    else:#if unknown\n",
        "        return dictionary[\"UNK\"]\n",
        "\n",
        "\n",
        "def prep_data():\n",
        "    input_text = list()\n",
        "    for textfile_path in [all_news_filepath, all_novel_filepath, all_article_filepath, all_encyclopedia_filepath]:\n",
        "        for input_file in textfile_path:\n",
        "            f = open(input_file,\"r\") #open file with name of \"*.txt\"\n",
        "            text = re.sub(r'\\|', ' ', f.read()) # replace separation symbol with white space           \n",
        "            text = re.sub(r'<\\W?\\w+>', '', text)# remove <NE> </NE> <AB> </AB> tags\n",
        "            text = text.split() #split() method without an argument splits on whitespace \n",
        "            indexed_text = list(map(lambda x:word_to_index(x), text[:max_length])) #map raw word string to its index   \n",
        "            if 'news' in input_file:\n",
        "                input_text.append([indexed_text,0]) \n",
        "            elif 'novel' in input_file:\n",
        "                input_text.append([indexed_text,1]) \n",
        "            elif 'article' in input_file:\n",
        "                input_text.append([indexed_text,2]) \n",
        "            elif 'encyclopedia' in input_file:\n",
        "                input_text.append([indexed_text,3]) \n",
        "            \n",
        "            f.close()\n",
        "    random.shuffle(input_text)\n",
        "    return input_text\n",
        "\n",
        "input_data = prep_data()\n",
        "train_data = input_data[:int(len(input_data)*0.6)]\n",
        "val_data = input_data[int(len(input_data)*0.6):int(len(input_data)*0.8)]\n",
        "test_data = input_data[int(len(input_data)*0.8):]\n",
        "\n",
        "train_input = [data[0] for data in train_data]\n",
        "train_input = sequence.pad_sequences(train_input, maxlen=max_length) #padding\n",
        "train_target = [data[1] for data in train_data]\n",
        "train_target=to_categorical(train_target, num_classes=4)\n",
        "\n",
        "val_input = [data[0] for data in val_data]\n",
        "val_input = sequence.pad_sequences(val_input, maxlen=max_length) #padding\n",
        "val_target = [data[1] for data in val_data]\n",
        "val_target=to_categorical(val_target, num_classes=4)\n",
        "\n",
        "test_input = [data[0] for data in test_data]\n",
        "test_input = sequence.pad_sequences(test_input, maxlen=max_length) #padding\n",
        "test_target = [data[1] for data in test_data]\n",
        "test_target=to_categorical(test_target, num_classes=4)\n",
        "\n",
        "del input_data, val_data,train_data, test_data"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syrKnUxWi-YI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52bdffcd-e9b4-4be8-af43-182a1ea43c31"
      },
      "source": [
        "#the classification model\n",
        "#TODO#5 find out how to initialize your embedding layer with pre-trained weights, evaluate and observe\n",
        "#don't forget to compare it with the same model that does not use pre-trained weights\n",
        "#you can use your own model too! and feel free to customize this model as you wish\n",
        "cls_model = Sequential()\n",
        "cls_model.add(Embedding(len(dictionary)+1, 32, input_length=max_length,mask_zero=True))\n",
        "cls_model.add(GRU(32))\n",
        "cls_model.add(Dropout(0.5))\n",
        "cls_model.add(Dense(4, activation='softmax'))\n",
        "opt=Adam(lr=0.01)\n",
        "cls_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "cls_model.summary()\n",
        "print('Train...')\n",
        "cls_model.fit(train_input, train_target,\n",
        "          epochs=10,\n",
        "          validation_data=[val_input, val_target])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 2000, 32)          9445280   \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 32)                6336      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 132       \n",
            "=================================================================\n",
            "Total params: 9,451,748\n",
            "Trainable params: 9,451,748\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train...\n",
            "Epoch 1/10\n",
            "10/10 [==============================] - 17s 1s/step - loss: 1.3633 - accuracy: 0.3273 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 22s 2s/step - loss: 1.2591 - accuracy: 0.4152 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 19s 2s/step - loss: 0.8099 - accuracy: 0.6957 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 19s 2s/step - loss: 0.4305 - accuracy: 0.8459 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 16s 2s/step - loss: 0.1156 - accuracy: 0.9821 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 20s 2s/step - loss: 0.0379 - accuracy: 0.9929 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 16s 2s/step - loss: 0.0381 - accuracy: 0.9938 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.0292 - accuracy: 0.9931 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 19s 2s/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 22s 2s/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc6914d03c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t_dK8l9H92h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fec7bee9-a5ae-45df-f93f-0d85154557f4"
      },
      "source": [
        "results = cls_model.evaluate(test_input, test_target)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 99ms/step - loss: 2.6134 - accuracy: 0.5000\n",
            "test loss, test acc: [2.613426446914673, 0.5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMFtC8Fkxk7E"
      },
      "source": [
        "from keras.initializers import Constant "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Chg07wohxEp0",
        "outputId": "6e993328-37c4-43e3-a9a7-ca2105891f3a"
      },
      "source": [
        "cls_model_embedding = Sequential()\r\n",
        "cls_model_embedding.add(Embedding(len(dictionary)+1, 32, input_length=max_length,mask_zero=True, embeddings_initializer=Constant(final_embeddings)))\r\n",
        "cls_model_embedding.add(GRU(32))\r\n",
        "cls_model_embedding.add(Dropout(0.5))\r\n",
        "cls_model_embedding.add(Dense(4, activation='softmax'))\r\n",
        "opt=Adam(lr=0.01)\r\n",
        "cls_model_embedding.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\r\n",
        "cls_model_embedding.summary()\r\n",
        "print('Train...')\r\n",
        "cls_model_embedding.fit(train_input, train_target,\r\n",
        "          epochs=10,\r\n",
        "          validation_data=[val_input, val_target])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 2000, 32)          9445280   \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (None, 32)                6336      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4)                 132       \n",
            "=================================================================\n",
            "Total params: 9,451,748\n",
            "Trainable params: 9,451,748\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train...\n",
            "Epoch 1/10\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.4531 - accuracy: 0.3261 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 16s 1s/step - loss: 1.1605 - accuracy: 0.4729 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 28s 3s/step - loss: 0.9503 - accuracy: 0.5797 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 19s 2s/step - loss: 0.7079 - accuracy: 0.7034 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 19s 2s/step - loss: 0.5623 - accuracy: 0.7862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 22s 2s/step - loss: 0.4943 - accuracy: 0.8390 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 19s 2s/step - loss: 0.3858 - accuracy: 0.8759 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 19s 2s/step - loss: 0.2056 - accuracy: 0.9447 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.1217 - accuracy: 0.9628 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 22s 2s/step - loss: 0.0902 - accuracy: 0.9776 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc68ff197f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTx30dNGAHMH",
        "outputId": "598aa412-67af-4337-a325-bb60d55b6cfd"
      },
      "source": [
        "results = cls_model_embedding.evaluate(test_input, test_target)\r\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 105ms/step - loss: 0.8444 - accuracy: 0.7549\n",
            "test loss, test acc: [0.8444308638572693, 0.7549019455909729]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}